{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64949aa2-e545-43fb-a7da-f535874cd8d9",
   "metadata": {},
   "source": [
    "##### heloYOLOv8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea74766-0627-4c0a-bfb5-9ea59374ea7f",
   "metadata": {},
   "source": [
    "##### jedenfalls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729206f6-12c5-47d8-a51e-7ae7cd2ff943",
   "metadata": {},
   "source": [
    "##### 15 Mar 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b107e5ab-160d-4e01-a632-51c429bc5655",
   "metadata": {},
   "source": [
    "##### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7ea826-bf54-41dc-bfa9-39bb1b973889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec92420-5d99-43c7-9385-22fe4dc9e6b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bebd6ac9-cf17-483d-a871-e46850bb1f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49c89c35-6ac1-4f78-b710-c578471de5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e76fc484-aa30-476d-bfde-bdb9158eae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cae7ef-2117-4bec-b045-297f9ffeccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f469278-0be1-4022-8cb6-688b8249b6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4162c4-1725-4639-ab80-58772a34786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477cfaec-ecc1-4ceb-bfdf-a866927ec42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "# Create some tensors\n",
    "a = tf.constant([[1.0,2.0,3.0],[4.0,5.0,6.0]])\n",
    "b = tf.constant([[1.0,2.0],[3.0,4.0],[5.0,6.0]])\n",
    "c = tf.matmula(a,b)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8efc5cd6-ce91-4ceb-b635-b270a51a4b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69a6589-f2cc-4d95-bf7e-e0c1e6f52aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a289cde-8c13-45e2-a250-dca937ce97bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f562d5a-f0dd-4513-9af6-e663363ae841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c47c17a4-3191-4531-bf0b-f67f0d983562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5e6930-1d90-4870-87ce-4fb54ed1e2f7",
   "metadata": {},
   "source": [
    "#### calculate mean and stdev of image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e694305f-61b6-42f0-af87-b0c831d3e391",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('./images/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f913c182-b07e-4259-a993-8f759f5b7082",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_path = './images/train'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffd9304-e72d-4394-9e3c-7cfd98572c73",
   "metadata": {},
   "source": [
    "##### resize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91e12226-cf3e-4d61-9b59-89acd0d4be24",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_transforms = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c3820c-8654-44b8-b20f-a41af019f33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.ImageFolder(root = training_dataset_path, transform = training_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb65bead-e440-46e9-ac4b-a4dca5ed345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size=32, shuffle-False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b9246394-b96f-4ecc-8c39-4b6433003fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_and_std(loader):\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    total_images_count = 0\n",
    "    for images, _ in loader:\n",
    "        image_count_in_a_batch = images.size(0)\n",
    "        images = images.view(image_count_in_a_batcfh, images.size(1), -1)\n",
    "        mean += images.means(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        total_images_count += images_count_in_a_batch\n",
    "    mean /= total_images_count\n",
    "    st /= total_images_count\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecbffd8-e0b6-40a0-b396-5a5b752dfd77",
   "metadata": {},
   "source": [
    "#### 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8542ba6-75bc-40e4-b9a1-3b31d554d2a1",
   "metadata": {},
   "source": [
    "#### 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ad87c8-b101-41cc-be7b-bf30b471f91a",
   "metadata": {},
   "source": [
    "#### 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618d53a9-4a65-4bd2-9b51-8c1a4af0fc1e",
   "metadata": {},
   "source": [
    "#### 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f329386-fb43-4471-8b8d-57132b15c0b3",
   "metadata": {},
   "source": [
    "#### load a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ce0c43d-4fc3-4cde-bed4-b98b59366600",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov8n.yaml\") # build a new model from scratch (8n is the smallest version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3fcf1b-9390-4f49-8826-56744ddb2627",
   "metadata": {},
   "source": [
    "#### train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9628917-17d6-4ad0-9323-3b212395c4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.train(\n",
    "    data=\"config.yaml\",\n",
    "    epochs=100\n",
    ") #train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1578fd7-f1d4-4544-8225-7d1762b4b04d",
   "metadata": {},
   "source": [
    "#### save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c15a53e-3cb9-497a-8d5b-a6f1934bf6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=H1pfPRQvO_w&list=PL3Dh_99BJkCEhE7Ri8W6aijiEqm3ZoGRq&index=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e081a63-15e7-47ed-b3e1-83752f5ce641",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18_model = model.resnet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f241108e-a412-4fa9-8a51-4fb2e2a72a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ftrs = resnet18.model.fc.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513dcb4c-9a93-4f7f-bd53-1328a26d1427",
   "metadata": {},
   "outputs": [],
   "source": [
    "number of classes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9585c70f-b8c2-4b94-be19-ef871830b126",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18_model.fc = nn.Linear(num_ftrs, number_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfd77e3-34fd-4c96-9ffe-766e5656b505",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18_model.load_state_dict(checkpoint['model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087cb67d-2524-47a5-9429-3b01c264e23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(resnet18_model, 'best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e83686-2ad3-4bb0-83c3-08294ffc4703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### load THE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f288f2ab-c909-4433-81ea-2ba0032de759",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [other_tree\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7578707e-5c56-4d7e-8f6c-a5492be0c496",
   "metadata": {},
   "outputs": [],
   "source": [
    "modele = torch.load('best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c8ccc2-e7ee-46e7-960a-2977ab933cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9558ca-b724-4c99-8013-55d7f0ad3d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "std2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ece33b-157e-4dae-b5f2-8878c4b94d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms2 = transforms.Componse([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(torch.Tensor(mean), torch.Tensor(std))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddd4b7b-38b0-4b36-9c66-4557503a7c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(model, image_transforms, image_path, clases):\n",
    "    model = model.eval()\n",
    "    image = Image.open(image_path)\n",
    "    image = image_transforms(image).float()\n",
    "    image = image.unsqueeze(0)\n",
    "\n",
    "    output = model(image)\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "    pritn(classes[predicted.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd74ad5-7768-46d4-ac04-606d0133c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(model, image_transforms, \"xxx.jpg\", classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
