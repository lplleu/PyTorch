{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3411ebfd-d8ce-4bb3-bc7f-0e8d250a6b9b",
   "metadata": {},
   "source": [
    "#### meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119e90b0-584a-4393-9820-6bf43994d3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tutorial: object detection on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b775ae-9752-4ae4-b565-399872ae068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jedenfalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979a3f20-e699-41a8-8b71-b046939712d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25 March 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b866d8a-dc23-4b59-9f6a-a51e79bcec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=z3kB3ISIPAg&list=PL3Dh_99BJkCEhE7Ri8W6aijiEqm3ZoGRq&index=4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb69d25-9d1e-4ae3-8206-2e880a933157",
   "metadata": {},
   "source": [
    "#### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac520be-3f48-48c9-b9eb-b475a5480363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a5d2e3-891b-405e-a38b-7651fd6f201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f348e5c-3954-4063-a300-d71f358bf2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1f00cc-2079-418d-aab7-4c176969ddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb17cbf-ad73-4f93-afa1-c286272d1b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4429e0f6-6e07-438f-abf1-9605213d9b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5043f999-4739-4ea2-ad4f-91a444a4fda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0cb7e9-473a-4cd3-aab1-30b6726106f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17ad1e1-2af7-419a-b701-b481f183a52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8dffd3-af84-4baa-9269-7a56a7e2aa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c68366b-db3d-4a2c-be45-f0d561918adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image as Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6759bf57-f355-49d2-a5be-c8240f2b3816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import natsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d01b7da-5d8e-42cd-bab0-a4880959c6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_path = './images/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504a7ea1-244e-4072-8ba1-18aedfb9364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_path = './images/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "80b46952-66e4-46c0-9241-46bfc49a6e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9c61b0-bc05-4f24-8abd-16aa0e9004ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('./images/train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab9ab2a-6356-495d-90af-3a46aaf9d692",
   "metadata": {},
   "source": [
    "#### train loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b297e3cb-cf33-4ed2-bf61-0c98d2888271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/how-to-load-images-without-using-imagefolder/59999/7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfa8a21-6d28-45d2-be5e-19b32c128959",
   "metadata": {},
   "source": [
    "#### transform images (to increase volume and variation of training dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98de7079-4907-4a7c-9283-b974b525b6ea",
   "metadata": {},
   "source": [
    "##### reduce size to speed up model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1d54f5-63c3-4b78-8968-aed4dc7fa1b0",
   "metadata": {},
   "source": [
    "##### images need to be same size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d07cf1c-6289-4963-966a-06e11b9268d0",
   "metadata": {},
   "source": [
    "##### random horizontal flip to increase number of images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ca2d81-9a76-4c9c-98dc-13f3d13e1ae1",
   "metadata": {},
   "source": [
    "##### load and preprocess data using PyTorchâ€™s DataLoader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6738156b-196e-4453-870b-01fb81cd03d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),   \n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c47a14f-569f-400f-8067-9542bd986b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_transforms = transforms.Compose([ #all the below transformations will be performed, and in the order indicated\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomCrop((224,224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5), #p means probability\n",
    "    transforms.RandomVerticalFlip(p=0.5), #p means probability\n",
    "    transforms.RandomRotation(10), #10 means 10 degrees\n",
    "    transforms.ColorJitter(hue=0.3),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    #transforms.Normalize(torch.Tensor(mean), torch.Tensor(std)), # meaning: image = (image - mean) / stdtransforms.ToTensor(), #generalisation of vectors and matrices (multidimensional array)\n",
    "    transforms.Normalize(torch.Tensor([0.5352, 0.5258, 0.4332]), torch.Tensor([0.2119, 0.1732, 0.1835])), # meaning: image = (image - mean) / stdtransforms.ToTensor(), #generalisation of vectors and matrices (multidimensional array)\n",
    "    transforms.ToTensor() #generalisation of vectors and matrices (multidimensional array)\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d353708-60e2-485b-90f2-c91a7db22a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self, main_dir, transform):\n",
    "        self.main_dir = main_dir\n",
    "        self.transform = transform\n",
    "        all_imgs = os.listdir(main_dir)\n",
    "        self.total_imgs = natsort.natsorted(all_imgs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.total_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_loc = os.path.join(self.main_dir, self.total_imgs[idx])\n",
    "        image = Image.open(img_loc).convert(\"RGB\")\n",
    "        tensor_image = self.transform(image)\n",
    "        return tensor_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca26142-e24f-4bfc-bd12-037176ead948",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = CustomDataSet(training_dataset_path, transform=training_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ff93b2-e994-4d8f-b1f2-0ed48b29c3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = CustomDataSet(training_dataset_path, training_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4994c35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset2 = CustomDataSet(training_dataset_path, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b004f215-45e1-415c-ab70-d828f82d4429",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(my_dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36144274-db37-4413-bd48-38a9ec9504ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = DatasetFolder(my_dataset,transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beeedfa-ef53-47ad-8e9f-d874bee27b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader = my_dataset.DataLoader(my_dataset , batch_size=5, shuffle=True) #, num_workers=4, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139f8b6a-eb94-437d-8d38-a9d7165dca4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DataLoader(my_dataset,shuffle=True,batch_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6be5a2-88db-4e55-b960-6753229b9a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = torchvision.datasets.ImageFolder(root='images/train', train=True, transform=transforms.To.Tensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992cab2b-ee99-4a07-a9e3-6dba49834435",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset.dataset, batch_size=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8cd5d0-09a3-48d7-a6bc-5a630fc8c31d",
   "metadata": {},
   "source": [
    "#### calculate mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2528a2b8-80b7-4177-b42d-5149deb6f131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_and_std(loader):\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    nb_samples = 0.\n",
    "    for data in loader:\n",
    "        batch_samples = data.size(0)\n",
    "        data = data.view(batch_samples, data.size(1), -1)\n",
    "        mean += data.mean(2).sum(0)\n",
    "        std += data.std(2).sum(0)\n",
    "        nb_samples += batch_samples\n",
    "    mean /= nb_samples\n",
    "    std /= nb_samples\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d525960-2410-484f-9f7e-d57cf467e313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_and_std2(loader):\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    total_images_count = 0\n",
    "    print(loader)\n",
    "\n",
    "    for image in loader:\n",
    "        image_count_in_batch = image.size(0)\n",
    "        #print(images.shape)\n",
    "        \n",
    "    \n",
    "        images = images.view(images_count_in_batch, images.size(1), -1)        \n",
    "        #print(images.shape)\n",
    "        \n",
    "        mean += images.mean(2).sum(0)\n",
    "        \n",
    "        std += images.std(2).sum(0)\n",
    "        total_images_count += image_count_in_batch\n",
    "        \n",
    "    mean /= total_images_count # there are 3 channels, for each channel find the mean\n",
    "    std /= total_images_count # there are 3 channels, for each channel find the stdev\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3f0842-ab0a-476c-bfc8-78590ade8ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_mean_and_std(my_dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5812c9d-aa1a-4305-8d32-c35cfd93c903",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_mean_and_std2(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2bf483-b535-442d-9fc2-02c81f8161e8",
   "metadata": {},
   "source": [
    "#### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c61549f-a27b-4ccc-b75b-adfb24a9d899",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### uses yolov8n which is the smallest one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5256d112-3a92-4226-9c4b-c108498d43e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov8n.yaml\") # build a new model from scratch (8n is the smallest version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843d881e-829c-4617-9453-0aedf8553b97",
   "metadata": {},
   "source": [
    "#### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12180d8b-dc17-459e-8dde-b7de1f52a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.train(\n",
    "    data=\"config.yaml\",\n",
    "    epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550f8348-d430-4c34-b5ec-61dfd9d17fca",
   "metadata": {},
   "source": [
    "#### save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167c62ff-b196-4efd-a011-d23a0bd182ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0810ee1-c03d-49ac-9fd1-1044bcec484a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47d4255b-1129-499d-83e4-39b0cd4a1dcf",
   "metadata": {},
   "source": [
    "#### load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd048471-9628-425a-9c30-2d1e807903af",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = torch.load('.\\runs\\detect\\train112\\weights\\best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4889f4e-8763-455a-baae-21b3120a31f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(), #generalisation of vectors and matrices (multidimensional array)\n",
    "    transforms.Normalize(torch.Tensor(mean), torch.Tensor(std))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4576d20-61ca-48b5-9230-8a5e106e2412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(loaded_model, image_transforms, image_path, classes):\n",
    "    model = loaded_model.eval()\n",
    "    image = Image.open(image_path)\n",
    "    image = image_transforms(image).float()\n",
    "    image = image.unsqueeze(0)\n",
    "\n",
    "    output = model(imag)\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "    print(classes[predicted.item()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02d33ef-ce28-4633-8ddc-e8612776ad2a",
   "metadata": {},
   "source": [
    "#### test model on a batch of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc2c154-7dde-4e5d-8fe7-ddfecd51473f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9213b04-1f2a-48d0-b3e8-55c22ec5bc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the filenames in the folder\n",
    "os.listdir('./images/val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce597f0-9c68-42fa-8d01-29128d805c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_image_path = './images/val/DJI_20240204094120_0258_D.JPG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e77c4f-785d-4f02-95b7-1041c9e59fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=detect mode=predict model=runs/detect/train112/weights/best.pt conf=0.25 source=images/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670d6b91-b7c6-44c2-88bd-0e5795952b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classify(model, image_transforms, \"pygm\", classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b90f9c-c751-48f1-b374-1b42dfc648e2",
   "metadata": {},
   "source": [
    "#### visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5542a977-600e-4fe5-b42a-eeb9dcb5d026",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83bc33d-0a28-425f-bd0d-e5994a0e4269",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e62bbb-d8aa-466f-8258-69ef1c9bd048",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feee58c2-5810-434f-a735-5149c9efdbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    for img in dataset: #, label\n",
    "        save_image(img, 'img'+str(img_num)+'.png') # visualise\n",
    "        img_num +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595f2449-81c5-4be3-8791-267ff2dc8b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92638d50-8965-405c-b811-3f60da337a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c13bac-0a09-4782-ae82-2c874793654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = './images/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff192abb-5367-4d3c-bb8d-31121103cd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path in glob.glob('runs/detect/predict6/*.JPG')[:10]:\n",
    "    display(Image(filename=image_path, width=600))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939d7982-7081-442b-a29e-73b0ca478906",
   "metadata": {},
   "source": [
    "#### classify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1f6c13-b705-4c9c-b356-2e9632038777",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cf0a34-f917-43ad-839b-a6799d84169e",
   "metadata": {},
   "source": [
    "#### measure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62064daf-9a2f-4e5d-81e5-4262bd96c46c",
   "metadata": {},
   "source": [
    "#### analyse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
